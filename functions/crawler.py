import re
import asyncio
import platform
import time  # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€
from urllib.parse import urljoin
from playwright.async_api import async_playwright
from functions.dict_lib import char_code, char_korean, weapon_korean
import json


class PatchNoteCrawler:
    """ìµœì í™”ëœ Eternal Return íŒ¨ì¹˜ë…¸íŠ¸ í¬ë¡¤ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.base_url = (
            "https://playeternalreturn.com/posts/news?categoryPath=patchnote"
        )
        # ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©ì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜
        self._browser = None
        self._context = None

    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì… - ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        self._playwright = await async_playwright().start()
        self._browser = await self._launch_browser()
        self._context = await self._browser.new_context(
            locale="ko-KR",
            # ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
            viewport={"width": 1280, "height": 720},
            ignore_https_errors=True,
            java_script_enabled=True,
            # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë¹„í™œì„±í™”
            bypass_csp=True,
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._context:
            await self._context.close()
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()

    async def get_patch_info(self) -> dict:
        """
        ìµœì í™”ëœ íŒ¨ì¹˜ë…¸íŠ¸ ì •ë³´ ìˆ˜ì§‘ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
        """
        crawling_results = {
            "major_patch_version": None,
            "major_patch_date": None,
            "major_patch_url": None,
            "minor_patch_data": [],
        }

        try:
            page = await self._context.new_page()

            # ì„±ëŠ¥ ìµœì í™”: ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
            await page.route(
                "**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2,ttf}",
                lambda route: route.abort(),
            )
            await page.route("**/analytics**", lambda route: route.abort())
            await page.route("**/ads**", lambda route: route.abort())

            # ë¹ ë¥¸ í˜ì´ì§€ ë¡œë“œ
            await self._load_page(page)

            # ë©”ì´ì € íŒ¨ì¹˜ ì •ë³´ ì¶”ì¶œ
            major_version, major_date, major_url = await self._extract_major_patch(page)
            crawling_results.update(
                {
                    "major_patch_version": major_version,
                    "major_patch_date": major_date,
                    "major_patch_url": major_url,
                }
            )

            # ë©”ì´ì € íŒ¨ì¹˜ë¥¼ ì°¾ì•˜ë‹¤ë©´ ë§ˆì´ë„ˆ íŒ¨ì¹˜ë„ ê²€ìƒ‰
            if major_version and major_url:
                minor_data = await self._extract_minor_patches(
                    page, major_version, major_url
                )
                crawling_results["minor_patch_data"] = minor_data

            await page.close()

        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

        return crawling_results

    async def _launch_browser(self):
        """ìµœì í™”ëœ ë¸Œë¼ìš°ì € ì„¤ì •"""
        current_os = platform.system()
        print(f"ìš´ì˜ì²´ì œ ê°ì§€: {current_os}")

        # ê³µí†µ ìµœì í™” ì˜µì…˜
        common_args = [
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--disable-web-security",
            "--disable-features=VizDisplayCompositor",
            "--disable-background-timer-throttling",
            "--disable-backgrounding-occluded-windows",
            "--disable-renderer-backgrounding",
            "--disable-extensions",
            "--disable-default-apps",
            "--disable-sync",
            "--disable-translate",
            "--disable-background-networking",
            "--disable-plugins",
            "--disable-print-preview",
            "--no-first-run",
            "--no-default-browser-check",
            "--single-process",  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì„
            "--memory-pressure-off",
        ]

        if current_os == "Linux":
            try:
                print("ë¦¬ëˆ…ìŠ¤ í™˜ê²½: Firefox ë¸Œë¼ìš°ì € ì‚¬ìš©")
                browser = await self._playwright.firefox.launch(
                    headless=True,
                    firefox_user_prefs={
                        "dom.webnotifications.enabled": False,
                        "dom.push.enabled": False,
                        "media.autoplay.enabled": False,
                        "permissions.default.image": 2,  # ì´ë¯¸ì§€ ì°¨ë‹¨
                    },
                )
            except Exception as e:
                print(f"Firefox ì‹¤í–‰ ì‹¤íŒ¨, Chromiumìœ¼ë¡œ ëŒ€ì²´ ì‹œë„: {e}")
                browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=common_args,
                )
        else:
            print(f"{current_os} í™˜ê²½: Chromium ë¸Œë¼ìš°ì € ì‚¬ìš©")
            browser = await self._playwright.chromium.launch(
                headless=True,
                args=common_args,
            )

        return browser

    async def _load_page(self, page):
        """ìµœì í™”ëœ í˜ì´ì§€ ë¡œë“œ"""
        print(f"í˜ì´ì§€ ë¡œë”© ì¤‘: {self.base_url}...")
        try:
            await page.goto(self.base_url, wait_until="domcontentloaded", timeout=8000)
            await page.wait_for_selector("h4.article-title", timeout=5000)
            print("í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë“œ ì¤‘ íƒ€ì„ì•„ì›ƒ: {e}")
            # íƒ€ì„ì•„ì›ƒì´ì–´ë„ ì´ë¯¸ ë¡œë“œëœ ì½˜í…ì¸ ë¡œ ì§„í–‰

    async def _extract_major_patch(self, page):
        """ìµœì í™”ëœ ë©”ì´ì € íŒ¨ì¹˜ ì¶”ì¶œ"""
        try:
            article_elements = await page.locator("h4.article-title").all()

            # ìƒìœ„ 10ê°œë§Œ ê²€ìƒ‰í•˜ì—¬ ì†ë„ í–¥ìƒ
            for title_locator in article_elements[:10]:
                title_text = await title_locator.text_content()

                if "PATCH NOTES" in title_text or "íŒ¨ì¹˜ë…¸íŠ¸" in title_text:
                    version_match = re.search(
                        r"(\d+\.\d+)\s*(?:PATCH NOTES|íŒ¨ì¹˜ë…¸íŠ¸)",
                        title_text,
                        re.IGNORECASE,
                    )
                    date_match = re.search(r"(\d{4}\.\d{2}\.\d{2})", title_text)

                    if version_match:
                        version = version_match.group(1)
                        date = date_match.group(1) if date_match else None

                        parent_a = title_locator.locator("xpath=ancestor::a[1]")
                        relative_url = await parent_a.get_attribute("href")

                        if relative_url:
                            url = urljoin(self.base_url, relative_url)
                            print(f"âœ… ë©”ì´ì € íŒ¨ì¹˜ ë°œê²¬: {version} - {url}")
                            return version, date, url

        except Exception as e:
            print(f"ë©”ì´ì € íŒ¨ì¹˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return None, None, None

    async def _extract_minor_patches(self, page, major_version, major_url):
        """ìµœì í™”ëœ ë§ˆì´ë„ˆ íŒ¨ì¹˜ ì¶”ì¶œ"""
        minor_patches = []
        minor_pattern = re.compile(rf"({re.escape(major_version)}[a-z])", re.IGNORECASE)

        try:
            article_elements = await page.locator("h4.article-title").all()

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ë¦¬ìŠ¤íŠ¸
            tasks = []
            for title_locator in article_elements[:20]:  # ìƒìœ„ 20ê°œë§Œ ê²€ìƒ‰
                tasks.append(
                    self._process_minor_patch(title_locator, minor_pattern, major_url)
                )

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘
            for result in list(reversed(results)):
                if isinstance(result, dict) and result:
                    minor_patches.append(result)
                    print(f"  - ë§ˆì´ë„ˆ íŒ¨ì¹˜ ë°œê²¬: {result['version']}")

        except Exception as e:
            print(f"ë§ˆì´ë„ˆ íŒ¨ì¹˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return list(reversed(minor_patches))  # ìµœì‹ ìˆœ ì •ë ¬

    async def _process_minor_patch(self, title_locator, minor_pattern, major_url):
        """ê°œë³„ ë§ˆì´ë„ˆ íŒ¨ì¹˜ ì²˜ë¦¬"""
        try:
            title_text = await title_locator.text_content()
            minor_match = minor_pattern.search(title_text)

            if minor_match:
                minor_version = minor_match.group(1)
                parent_a = title_locator.locator("xpath=ancestor::a[1]")
                relative_url = await parent_a.get_attribute("href")

                if relative_url:
                    url = urljoin(self.base_url, relative_url)
                    if url != major_url:  # ì¤‘ë³µ ë°©ì§€
                        return {"version": minor_version, "url": url}
        except Exception:
            pass
        return None


class StatisticsCrawler(PatchNoteCrawler):
    """
    PatchNoteCrawlerë¥¼ ìƒì†ë°›ì•„ ê²Œì„ í†µê³„ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
    ê¸°ì¡´ ER_statistics.pyì˜ dakgg_crawler í•¨ìˆ˜ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        super().__init__()
        # í†µê³„ í¬ë¡¤ë§ìš© ê¸°ë³¸ URLì€ ë‹¤ë¦„
        self.base_url = "https://dak.gg"
        # í˜ì´ì§€ ìºì‹±ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self._current_page = None
        self._current_url = None
        self._page_loaded = False

    async def _get_or_create_page(self, url):
        """í˜ì´ì§€ ì¬ì‚¬ìš© ë˜ëŠ” ìƒˆ í˜ì´ì§€ ìƒì„±"""
        # ê°™ì€ URLì´ê³  í˜ì´ì§€ê°€ ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ì¬ì‚¬ìš©
        if self._current_page and self._current_url == url and self._page_loaded:
            print(f"ğŸ”„ ê¸°ì¡´ í˜ì´ì§€ ì¬ì‚¬ìš©: {url}")
            return self._current_page

        # ê¸°ì¡´ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ë‹«ê¸°
        if self._current_page:
            try:
                await self._current_page.close()
            except:
                pass

        # ìƒˆ í˜ì´ì§€ ìƒì„±
        print(f"ğŸ†• ìƒˆ í˜ì´ì§€ ìƒì„±: {url}")
        self._current_page = await self._context.new_page()

        # ì„±ëŠ¥ ìµœì í™”: ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (ë” ê°•í™”)
        await self._current_page.route(
            "**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2,ttf,css}",
            lambda route: route.abort(),
        )
        await self._current_page.route("**/analytics**", lambda route: route.abort())
        await self._current_page.route("**/ads**", lambda route: route.abort())
        await self._current_page.route("**/tracking**", lambda route: route.abort())
        await self._current_page.route("**/gtag**", lambda route: route.abort())

        # ë¹ ë¥¸ í˜ì´ì§€ ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
        print(f"â³ í˜ì´ì§€ ë¡œë”© ì‹œì‘...")
        start_time = time.time()

        await self._current_page.goto(url, wait_until="domcontentloaded", timeout=8000)

        # í†µê³„ ì„¹ì…˜ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
        await self._current_page.wait_for_selector("div.css-n6szh2", timeout=6000)

        load_time = time.time() - start_time
        print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {load_time:.2f}ì´ˆ")

        self._current_url = url
        self._page_loaded = True

        return self._current_page

    async def dakgg_crawler(self, weapon, character_name):
        """
        ë¬´ê¸°, ìºë¦­í„° ì´ë¦„ìœ¼ë¡œ ë‹¥ì§€ì§€ í†µê³„ í¬ë¡¤ë§í•´ì˜¤ê¸°
        ê¸°ì¡´ ER_statistics.pyì˜ dakgg_crawler í•¨ìˆ˜ì™€ ë™ì¼í•œ ë°˜í™˜ê°’ í˜•ì‹ ìœ ì§€

        ë°˜í™˜ê°’ : dict(str(ê° í†µê³„ html)) /
        ì—ëŸ¬ì‹œ aiohttp.ClientResponseErrorì™€ ìœ ì‚¬í•œ ì˜ˆì™¸ ì²˜ë¦¬
        """
        url = f"https://dak.gg/er/characters/{character_name}?weaponType={weapon}"

        try:
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™” (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
            statistics_dict = {
                "code": char_code.get(character_name, 0),
                "character_name": char_korean.get(character_name, character_name),
                "weapon": weapon_korean.get(weapon, weapon),
            }

            page = await self._get_or_create_page(url)

            # í†µê³„ ë°ì´í„° ì¶”ì¶œ
            start_time = time.time()
            stat_elements = await page.locator("div.css-n6szh2").all()

            for stat_block in stat_elements:
                try:
                    # í†µê³„ ì´ë¦„ ì¶”ì¶œ
                    stat_name_element = stat_block.locator("div.css-dy7q68")
                    stat_name = await stat_name_element.inner_text(timeout=2000)
                    stat_name = stat_name.strip()

                    if not stat_name:
                        continue

                    # ê°’ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
                    value_element = stat_block.locator("div.css-1s2413a")
                    value = ""

                    try:
                        # div.css-1s2413aê°€ ìˆëŠ” ê²½ìš°
                        value = await value_element.inner_text(timeout=1500)
                        value = value.replace("%", " %")
                    except:
                        # span íƒœê·¸ê°€ ìˆëŠ” ê²½ìš°
                        try:
                            span_element = stat_block.locator("span")
                            value = await span_element.inner_text(timeout=1500)
                        except:
                            value = ""

                    # ìˆœìœ„ ì •ë³´ ì¶”ì¶œ
                    ranking_element = stat_block.locator("div.css-1sw8f3s")
                    ranking = ""
                    try:
                        ranking = await ranking_element.inner_text(timeout=1500)
                        ranking = ranking.replace("#", "")
                    except:
                        ranking = ""

                    # ê¸°ì¡´ í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
                    statistics_dict[stat_name] = {
                        "value": value.strip(),
                        "ranking": ranking.strip(),
                    }

                except Exception as e:
                    print(f"âš ï¸ í†µê³„ ë¸”ë¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            extract_time = time.time() - start_time
            print(f"ğŸ“Š ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {extract_time:.2f}ì´ˆ")
            print(f"âœ… í†µê³„ í¬ë¡¤ë§ ì™„ë£Œ: {character_name} + {weapon}")

            return statistics_dict

        except Exception as e:
            print(f"âŒ dak.gg í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            # ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ ë°œìƒ
            from aiohttp import ClientResponseError

            raise ClientResponseError(
                request_info=None,
                history=(),
                status=500,
                message=f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}",
            )

    async def scrape_tier_statistics(self, weapon, character_name, tier="ë‹¤ì´ì•„ëª¬ë“œ+"):
        """
        íŠ¹ì • í‹°ì–´ì˜ í†µê³„ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ (ìµœì í™”ëœ ë²„ì „)
        ê¸°ì¡´ í˜ì´ì§€ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ë¡œë”© ì‹œê°„ ëŒ€í­ ë‹¨ì¶•
        """

        url = f"https://dak.gg/er/characters/{character_name}?weaponType={weapon}"

        try:
            # í˜ì´ì§€ ì¬ì‚¬ìš© ë˜ëŠ” ìƒì„±
            page = await self._get_or_create_page(url)

            # í‹°ì–´ ë“œë¡­ë‹¤ìš´ í´ë¦­ (ìµœì í™”ëœ ì…€ë ‰í„°ì™€ ë‹¨ì¶•ëœ íƒ€ì„ì•„ì›ƒ)
            tier_change_start = time.time()

            try:
                print(f"ğŸ¯ '{tier}' í‹°ì–´ë¡œ ë³€ê²½ ì‹œë„...")

                # ë” íš¨ìœ¨ì ì¸ ì…€ë ‰í„° ìˆœì„œ (ì„±ê³µë¥  ë†’ì€ ê²ƒë¶€í„°)
                tier_selectors = [
                    "button.css-1lyboqe",  # ê°€ì¥ ì¼ë°˜ì ì¸ í˜•íƒœ
                    "button[class*='trigger']",
                    ".tier-selector button",
                    "button:has-text('ë‹¤ì´ì•„ëª¬ë“œ+')",
                ]

                # ë“œë¡­ë‹¤ìš´ í´ë¦­
                clicked = False
                for selector in tier_selectors:
                    try:
                        await page.click(selector, timeout=2000)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                        print(f"ğŸ¯ ë“œë¡­ë‹¤ìš´ í´ë¦­ ì„±ê³µ: {selector}")
                        clicked = True
                        break
                    except:
                        continue

                if not clicked:
                    print(f"âš ï¸ ë“œë¡­ë‹¤ìš´ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ í†µê³„ ì‚¬ìš©")
                    return await self._extract_statistics_data(
                        page, weapon, character_name
                    )

                # ë“œë¡­ë‹¤ìš´ ì—´ë¦¼ ëŒ€ê¸° (ì‹œê°„ ë‹¨ì¶•)
                await asyncio.sleep(0.5)  # 1.5ì´ˆì—ì„œ 0.5ì´ˆë¡œ ë‹¨ì¶•

                # í‹°ì–´ ì˜µì…˜ í´ë¦­ (ë” ì •í™•í•œ ì…€ë ‰í„°)
                tier_option_selectors = [
                    f"li:has-text('{tier}')",
                    f"[role='option']:has-text('{tier}')",
                    f".option-item:has-text('{tier}')",
                    f"div[data-value='{tier}']",
                ]

                tier_selected = False
                for option_selector in tier_option_selectors:
                    try:
                        await page.click(option_selector, timeout=2000)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                        print(f"âœ… '{tier}' í‹°ì–´ ì„ íƒ ì„±ê³µ")
                        tier_selected = True
                        break
                    except:
                        continue

                if not tier_selected:
                    print(f"âš ï¸ '{tier}' í‹°ì–´ ì˜µì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return await self._extract_statistics_data(
                        page, weapon, character_name
                    )

                # í˜ì´ì§€ ì—…ë°ì´íŠ¸ ëŒ€ê¸° (ìµœì í™”)
                print(f"â³ ë°ì´í„° ì—…ë°ì´íŠ¸ ëŒ€ê¸°...")

                # ë” íš¨ìœ¨ì ì¸ ëŒ€ê¸° ë°©ì‹: íŠ¹ì • ìš”ì†Œì˜ ë³€ê²½ì„ ê°ì§€
                try:
                    # í†µê³„ ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    await page.wait_for_function(
                        """() => {
                            const elements = document.querySelectorAll('div.css-n6szh2');
                            return elements.length > 0;
                        }""",
                        timeout=3000,  # 5ì´ˆì—ì„œ 3ì´ˆë¡œ ë‹¨ì¶•
                    )

                    # ì¶”ê°€ì ì¸ ì‘ì€ ëŒ€ê¸° (DOM ì—…ë°ì´íŠ¸ ì™„ë£Œ ë³´ì¥)
                    await asyncio.sleep(0.8)  # 3ì´ˆì—ì„œ 0.8ì´ˆë¡œ ëŒ€í­ ë‹¨ì¶•

                except Exception as wait_error:
                    print(f"âš ï¸ í˜ì´ì§€ ì—…ë°ì´íŠ¸ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ: {wait_error}")
                    # íƒ€ì„ì•„ì›ƒì´ì–´ë„ ì§„í–‰

                tier_change_time = time.time() - tier_change_start
                print(f"ğŸ í‹°ì–´ ë³€ê²½ ì™„ë£Œ: {tier_change_time:.2f}ì´ˆ")

            except Exception as e:
                print(f"âš ï¸ í‹°ì–´ ì„ íƒ ì¤‘ ì˜¤ë¥˜ (ê¸°ë³¸ í†µê³„ ì‚¬ìš©): {e}")

            # í†µê³„ ë°ì´í„° ì¶”ì¶œ
            statistics_dict = await self._extract_statistics_data(
                page, weapon, character_name
            )

            print(f"ğŸ‰ í‹°ì–´ë³„ í†µê³„ í¬ë¡¤ë§ ì™„ë£Œ: {character_name} + {weapon} ({tier})")

            return statistics_dict

        except Exception as e:
            print(f"âŒ í‹°ì–´ë³„ í†µê³„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í†µê³„ë¡œ í´ë°±
            return await self.dakgg_crawler(weapon, character_name)

    async def _extract_statistics_data(self, page, weapon, character_name):
        """í†µê³„ ë°ì´í„° ì¶”ì¶œ ê³µí†µ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)"""
        start_time = time.time()

        statistics_dict = {
            "code": char_code.get(character_name, 0),
            "character_name": char_korean.get(character_name, character_name),
            "weapon": weapon_korean.get(weapon, weapon),
        }

        try:
            # í†µê³„ ì„¹ì…˜ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
            await page.wait_for_selector("div.css-n6szh2", timeout=4000)

            # í†µê³„ ë°ì´í„° ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ìµœì í™”)
            stat_elements = await page.locator("div.css-n6szh2").all()

            # ê° í†µê³„ ë¸”ë¡ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            tasks = []
            for stat_block in stat_elements:
                tasks.append(self._extract_single_stat(stat_block))

            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ì •ë¦¬
            for result in results:
                if isinstance(result, dict) and result.get("name"):
                    stat_name = result["name"]
                    statistics_dict[stat_name] = {
                        "value": result.get("value", "").strip(),
                        "ranking": result.get("ranking", "").strip(),
                    }

            extract_time = time.time() - start_time
            print(f"ğŸ“Š ë°ì´í„° ì¶”ì¶œ ì‹œê°„: {extract_time:.2f}ì´ˆ")

        except Exception as e:
            print(f"âš ï¸ í†µê³„ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return statistics_dict

    async def _extract_single_stat(self, stat_block):
        """ë‹¨ì¼ í†µê³„ ë¸”ë¡ì—ì„œ ë°ì´í„° ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            # í†µê³„ ì´ë¦„ ì¶”ì¶œ
            stat_name_element = stat_block.locator("div.css-dy7q68")
            stat_name = await stat_name_element.inner_text(timeout=1500)
            stat_name = stat_name.strip()

            if not stat_name:
                return {}

            # ê°’ê³¼ ìˆœìœ„ë¥¼ ë³‘ë ¬ë¡œ ì¶”ì¶œ
            value_task = self._extract_value(stat_block)
            ranking_task = self._extract_ranking(stat_block)

            value, ranking = await asyncio.gather(
                value_task, ranking_task, return_exceptions=True
            )

            return {
                "name": stat_name,
                "value": value if isinstance(value, str) else "",
                "ranking": ranking if isinstance(ranking, str) else "",
            }

        except Exception as e:
            return {}

    async def _extract_value(self, stat_block):
        """ê°’ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
        try:
            value_element = stat_block.locator("div.css-1s2413a")
            value = await value_element.inner_text(timeout=1000)
            return value.replace("%", " %")
        except:
            try:
                span_element = stat_block.locator("span")
                return await span_element.inner_text(timeout=1000)
            except:
                return ""

    async def _extract_ranking(self, stat_block):
        """ìˆœìœ„ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
        try:
            ranking_element = stat_block.locator("div.css-1sw8f3s")
            ranking = await ranking_element.inner_text(timeout=1000)
            return ranking.replace("#", "")
        except:
            return ""

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ ì‹œ í˜ì´ì§€ë„ ì •ë¦¬"""
        if self._current_page:
            try:
                await self._current_page.close()
            except:
                pass

        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ì •ë¦¬ ë©”ì„œë“œ í˜¸ì¶œ
        await super().__aexit__(exc_type, exc_val, exc_tb)
