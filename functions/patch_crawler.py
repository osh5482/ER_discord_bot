import re
import asyncio
import platform
from urllib.parse import urljoin
from playwright.async_api import async_playwright


class PatchNoteCrawler:
    """ìµœì í™”ëœ Eternal Return íŒ¨ì¹˜ë…¸íŠ¸ í¬ë¡¤ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.base_url = (
            "https://playeternalreturn.com/posts/news?categoryPath=patchnote"
        )
        # ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©ì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜
        self._browser = None
        self._context = None

    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì… - ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        self._playwright = await async_playwright().start()
        self._browser = await self._launch_browser()
        self._context = await self._browser.new_context(
            locale="ko-KR",
            # ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
            viewport={"width": 1280, "height": 720},
            ignore_https_errors=True,
            java_script_enabled=True,
            # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë¹„í™œì„±í™”
            bypass_csp=True,
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._context:
            await self._context.close()
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()

    async def get_patch_info(self) -> dict:
        """
        ìµœì í™”ëœ íŒ¨ì¹˜ë…¸íŠ¸ ì •ë³´ ìˆ˜ì§‘ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
        """
        crawling_results = {
            "major_patch_version": None,
            "major_patch_date": None,
            "major_patch_url": None,
            "minor_patch_data": [],
        }

        try:
            page = await self._context.new_page()

            # ì„±ëŠ¥ ìµœì í™”: ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
            await page.route(
                "**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2,ttf}",
                lambda route: route.abort(),
            )
            await page.route("**/analytics**", lambda route: route.abort())
            await page.route("**/ads**", lambda route: route.abort())

            # ë¹ ë¥¸ í˜ì´ì§€ ë¡œë“œ
            await self._load_page(page)

            # ë©”ì´ì € íŒ¨ì¹˜ ì •ë³´ ì¶”ì¶œ
            major_version, major_date, major_url = await self._extract_major_patch(page)
            crawling_results.update(
                {
                    "major_patch_version": major_version,
                    "major_patch_date": major_date,
                    "major_patch_url": major_url,
                }
            )

            # ë©”ì´ì € íŒ¨ì¹˜ë¥¼ ì°¾ì•˜ë‹¤ë©´ ë§ˆì´ë„ˆ íŒ¨ì¹˜ë„ ê²€ìƒ‰
            if major_version and major_url:
                minor_data = await self._extract_minor_patches(
                    page, major_version, major_url
                )
                crawling_results["minor_patch_data"] = minor_data

            await page.close()

        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

        return crawling_results

    async def _launch_browser(self):
        """ìµœì í™”ëœ ë¸Œë¼ìš°ì € ì„¤ì •"""
        current_os = platform.system()
        print(f"ìš´ì˜ì²´ì œ ê°ì§€: {current_os}")

        # ê³µí†µ ìµœì í™” ì˜µì…˜
        common_args = [
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--disable-web-security",
            "--disable-features=VizDisplayCompositor",
            "--disable-background-timer-throttling",
            "--disable-backgrounding-occluded-windows",
            "--disable-renderer-backgrounding",
            "--disable-extensions",
            "--disable-default-apps",
            "--disable-sync",
            "--disable-translate",
            "--disable-background-networking",
            "--disable-plugins",
            "--disable-print-preview",
            "--no-first-run",
            "--no-default-browser-check",
            "--single-process",  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì„
            "--memory-pressure-off",
        ]

        if current_os == "Linux":
            try:
                print("ë¦¬ëˆ…ìŠ¤ í™˜ê²½: Firefox ë¸Œë¼ìš°ì € ì‚¬ìš©")
                browser = await self._playwright.firefox.launch(
                    headless=True,
                    firefox_user_prefs={
                        "dom.webnotifications.enabled": False,
                        "dom.push.enabled": False,
                        "media.autoplay.enabled": False,
                        "permissions.default.image": 2,  # ì´ë¯¸ì§€ ì°¨ë‹¨
                    },
                )
            except Exception as e:
                print(f"Firefox ì‹¤í–‰ ì‹¤íŒ¨, Chromiumìœ¼ë¡œ ëŒ€ì²´ ì‹œë„: {e}")
                browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=common_args,
                )
        else:
            print(f"{current_os} í™˜ê²½: Chromium ë¸Œë¼ìš°ì € ì‚¬ìš©")
            browser = await self._playwright.chromium.launch(
                headless=True,
                args=common_args,
            )

        return browser

    async def _load_page(self, page):
        """ìµœì í™”ëœ í˜ì´ì§€ ë¡œë“œ"""
        print(f"í˜ì´ì§€ ë¡œë”© ì¤‘: {self.base_url}...")
        try:
            await page.goto(self.base_url, wait_until="domcontentloaded", timeout=8000)
            await page.wait_for_selector("h4.article-title", timeout=5000)
            print("í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë“œ ì¤‘ íƒ€ì„ì•„ì›ƒ: {e}")
            # íƒ€ì„ì•„ì›ƒì´ì–´ë„ ì´ë¯¸ ë¡œë“œëœ ì½˜í…ì¸ ë¡œ ì§„í–‰

    async def _extract_major_patch(self, page):
        """ìµœì í™”ëœ ë©”ì´ì € íŒ¨ì¹˜ ì¶”ì¶œ"""
        try:
            article_elements = await page.locator("h4.article-title").all()

            # ìƒìœ„ 10ê°œë§Œ ê²€ìƒ‰í•˜ì—¬ ì†ë„ í–¥ìƒ
            for title_locator in article_elements[:10]:
                title_text = await title_locator.text_content()

                if "PATCH NOTES" in title_text or "íŒ¨ì¹˜ë…¸íŠ¸" in title_text:
                    version_match = re.search(
                        r"(\d+\.\d+)\s*(?:PATCH NOTES|íŒ¨ì¹˜ë…¸íŠ¸)",
                        title_text,
                        re.IGNORECASE,
                    )
                    date_match = re.search(r"(\d{4}\.\d{2}\.\d{2})", title_text)

                    if version_match:
                        version = version_match.group(1)
                        date = date_match.group(1) if date_match else None

                        parent_a = title_locator.locator("xpath=ancestor::a[1]")
                        relative_url = await parent_a.get_attribute("href")

                        if relative_url:
                            url = urljoin(self.base_url, relative_url)
                            print(f"âœ… ë©”ì´ì € íŒ¨ì¹˜ ë°œê²¬: {version} - {url}")
                            return version, date, url

        except Exception as e:
            print(f"ë©”ì´ì € íŒ¨ì¹˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return None, None, None

    async def _extract_minor_patches(self, page, major_version, major_url):
        """ìµœì í™”ëœ ë§ˆì´ë„ˆ íŒ¨ì¹˜ ì¶”ì¶œ"""
        minor_patches = []
        minor_pattern = re.compile(rf"({re.escape(major_version)}[a-z])", re.IGNORECASE)

        try:
            article_elements = await page.locator("h4.article-title").all()

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ë¦¬ìŠ¤íŠ¸
            tasks = []
            for title_locator in article_elements[:20]:  # ìƒìœ„ 20ê°œë§Œ ê²€ìƒ‰
                tasks.append(
                    self._process_minor_patch(title_locator, minor_pattern, major_url)
                )

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘
            for result in results:
                if isinstance(result, dict) and result:
                    minor_patches.append(result)
                    print(f"  - ë§ˆì´ë„ˆ íŒ¨ì¹˜ ë°œê²¬: {result['version']}")

        except Exception as e:
            print(f"ë§ˆì´ë„ˆ íŒ¨ì¹˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return list(reversed(minor_patches))  # ìµœì‹ ìˆœ ì •ë ¬

    async def _process_minor_patch(self, title_locator, minor_pattern, major_url):
        """ê°œë³„ ë§ˆì´ë„ˆ íŒ¨ì¹˜ ì²˜ë¦¬"""
        try:
            title_text = await title_locator.text_content()
            minor_match = minor_pattern.search(title_text)

            if minor_match:
                minor_version = minor_match.group(1)
                parent_a = title_locator.locator("xpath=ancestor::a[1]")
                relative_url = await parent_a.get_attribute("href")

                if relative_url:
                    url = urljoin(self.base_url, relative_url)
                    if url != major_url:  # ì¤‘ë³µ ë°©ì§€
                        return {"version": minor_version, "url": url}
        except Exception:
            pass
        return None


# ìºì‹± ì‹œìŠ¤í…œ
_patch_cache = {"data": None, "timestamp": 0, "cache_duration": 300}  # 5ë¶„ ìºì‹œ


async def get_cached_patch_info():
    """ìºì‹œëœ íŒ¨ì¹˜ë…¸íŠ¸ ì •ë³´ ë°˜í™˜"""
    import time

    current_time = time.time()

    # ìºì‹œ ìœ íš¨ì„± í™•ì¸
    if (
        _patch_cache["data"]
        and current_time - _patch_cache["timestamp"] < _patch_cache["cache_duration"]
    ):
        print("âœ… ìºì‹œëœ íŒ¨ì¹˜ë…¸íŠ¸ ì •ë³´ ì‚¬ìš©")
        return _patch_cache["data"]

    # ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‹¤í–‰
    print("ğŸ”„ ìƒˆë¡œìš´ íŒ¨ì¹˜ë…¸íŠ¸ ì •ë³´ í¬ë¡¤ë§...")
    async with PatchNoteCrawler() as crawler:
        patch_info = await crawler.get_patch_info()

        if patch_info:
            _patch_cache["data"] = patch_info
            _patch_cache["timestamp"] = current_time
            print("âœ… íŒ¨ì¹˜ë…¸íŠ¸ ì •ë³´ ìºì‹œ ì—…ë°ì´íŠ¸")

        return patch_info
